{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "from tqdm import tqdm\n",
    "import random as rd\n",
    "\n",
    "import torch_geometric.nn as G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_linear_program(num_variables, num_constraints, nnz = 5):\n",
    "    # Generate random coefficients for the objective function\n",
    "    c = np.random.uniform(-1, 1, num_variables) * 0.01\n",
    "\n",
    "    # Generate random coefficients for the constraint matrix\n",
    "    A = np.zeros((num_constraints, num_variables))\n",
    "    EdgeIndex = np.zeros((nnz, 2))\n",
    "    EdgeIndex1D = rd.sample(range(num_constraints * num_variables), nnz)\n",
    "    EdgeFeature = np.random.normal(0, 1, nnz)\n",
    "        \n",
    "    for l in range(nnz):\n",
    "        i = int(EdgeIndex1D[l] / num_variables)\n",
    "        j = EdgeIndex1D[l] - i * num_variables\n",
    "        EdgeIndex[l, 0] = i\n",
    "        EdgeIndex[l, 1] = j\n",
    "        A[i, j] = EdgeFeature[l]\n",
    "\n",
    "    # Generate random right-hand side values for the constraints\n",
    "    b = np.random.uniform(-1, 1, num_constraints)\n",
    "\n",
    "    bounds = np.random.normal(0, 10, size = (num_variables, 2))\n",
    "        \n",
    "    for j in range(num_variables):\n",
    "        if bounds[j, 0] > bounds[j, 1]:\n",
    "            temp = bounds[j, 0]\n",
    "            bounds[j, 0] = bounds[j, 1]\n",
    "            bounds[j, 1] = temp\n",
    "\n",
    "    # Generate random constraint types (0 for <= and 1 for =)\n",
    "    constraint_types = np.random.choice([0, 1], size=num_constraints, p=[0.7, 0.3])\n",
    "\n",
    "    # Adjust the constraint matrix and right-hand side based on the constraint types\n",
    "    for i in range(num_constraints):\n",
    "        if constraint_types[i] == 1:  # Equality constraint\n",
    "            b[i] = np.dot(A[i], np.random.rand(num_variables))  # Randomly generate a feasible solution\n",
    "\n",
    "    # Return the generated linear program\n",
    "    return c, A, b, constraint_types, bounds\n",
    "\n",
    "\n",
    "def solve_linear_program(c, A, b, constraint_types, bounds):\n",
    "    # Solve the linear program\n",
    "    res = linprog(c, A_ub=A[constraint_types == 0], b_ub=b[constraint_types == 0],\n",
    "                  A_eq=A[constraint_types == 1], b_eq=b[constraint_types == 1], bounds=bounds)\n",
    "\n",
    "    # Return the solution\n",
    "    return res.x, res.status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_solve_batches(num_batches, num_variables, num_constraints, out_func):\n",
    "    batches_c = []\n",
    "    batches_A = []\n",
    "    batches_b = []\n",
    "    batches_constraint_types = []\n",
    "    batches_lower_bounds = []\n",
    "    batches_upper_bounds = []\n",
    "    batches_solutions = []\n",
    "    batches_feasibility = []\n",
    "    \n",
    "    if out_func == 'feas':\n",
    "        for _ in range(num_batches):\n",
    "            c, A, b, constraint_types, bounds = generate_random_linear_program(num_variables,\n",
    "                                                                               num_constraints)\n",
    "\n",
    "            solution, feasibility = solve_linear_program(c, A, b, constraint_types, bounds)\n",
    "\n",
    "            lower_bounds, upper_bounds = zip(*bounds)\n",
    "\n",
    "            batches_c.append(torch.tensor(c, dtype=torch.float32))\n",
    "            batches_A.append(torch.tensor(A, dtype=torch.float32))\n",
    "            batches_b.append(torch.tensor(b, dtype=torch.float32))\n",
    "            batches_constraint_types.append(torch.tensor(constraint_types, dtype=torch.float32))\n",
    "            batches_lower_bounds.append(torch.tensor(lower_bounds, dtype=torch.float32))\n",
    "            batches_upper_bounds.append(torch.tensor(upper_bounds, dtype=torch.float32))\n",
    "\n",
    "            if type(solution) != type(None):\n",
    "                batches_solutions.append(torch.tensor(solution, dtype=torch.float32))\n",
    "\n",
    "            else:\n",
    "                batches_solutions.append(torch.zeros(num_variables, dtype=torch.float32))\n",
    "\n",
    "            batches_feasibility.append(torch.tensor(1 if feasibility != 2 else 0, dtype=torch.float32))\n",
    "\n",
    "        return (\n",
    "            torch.stack(batches_c),\n",
    "            torch.stack(batches_A),\n",
    "            torch.stack(batches_b),\n",
    "            torch.stack(batches_constraint_types),\n",
    "            torch.stack(batches_lower_bounds),\n",
    "            torch.stack(batches_upper_bounds),\n",
    "            torch.stack(batches_solutions),\n",
    "            torch.stack(batches_feasibility)\n",
    "        )\n",
    "    else:\n",
    "        while len(batches_c) != num_batches:\n",
    "            c, A, b, constraint_types, bounds = generate_random_linear_program(num_variables,\n",
    "                                                                               num_constraints)\n",
    "            solution, feasibility = solve_linear_program(c, A, b, constraint_types, bounds)\n",
    "\n",
    "            if (type(solution) == type(None)):\n",
    "                continue\n",
    "            \n",
    "            lower_bounds, upper_bounds = zip(*bounds)\n",
    "\n",
    "            batches_c.append(torch.tensor(c, dtype=torch.float32))\n",
    "            batches_A.append(torch.tensor(A, dtype=torch.float32))\n",
    "            batches_b.append(torch.tensor(b, dtype=torch.float32))\n",
    "            batches_constraint_types.append(torch.tensor(constraint_types, dtype=torch.float32))\n",
    "            batches_lower_bounds.append(torch.tensor(lower_bounds, dtype=torch.float32))\n",
    "            batches_upper_bounds.append(torch.tensor(upper_bounds, dtype=torch.float32))\n",
    "            batches_solutions.append(torch.tensor(solution, dtype=torch.float32))\n",
    "            batches_feasibility.append(torch.tensor(1 if feasibility != 2 else 0, dtype=torch.float32))\n",
    "\n",
    "        return (\n",
    "            torch.stack(batches_c),\n",
    "            torch.stack(batches_A),\n",
    "            torch.stack(batches_b),\n",
    "            torch.stack(batches_constraint_types),\n",
    "            torch.stack(batches_lower_bounds),\n",
    "            torch.stack(batches_upper_bounds),\n",
    "            torch.stack(batches_solutions),\n",
    "            torch.stack(batches_feasibility)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LPGNN(nn.Module):\n",
    "    def __init__(self, num_constraints, num_variables):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_constraints = num_constraints\n",
    "        self.num_variables = num_variables\n",
    "\n",
    "        self.num_layers = 5\n",
    "        ints = np.random.randint(1, 10, size=self.num_layers)\n",
    "        dims = [2 ** i for i in ints]\n",
    "        \n",
    "        # Encode the input features into the embedding space\n",
    "        self.fv_in = G.MLP([2, 32, dims[0]])\n",
    "        self.fw_in = G.MLP([3, 32, dims[0]])\n",
    "\n",
    "        # Hidden states of left nodes\n",
    "        self.fv = nn.ModuleList([G.MLP([dims[l-1], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "        self.gv = nn.ModuleList([G.MLP([dims[l-1] + dims[l], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "\n",
    "        # Hidden states of right nodes\n",
    "        self.fw = nn.ModuleList([G.MLP([dims[l-1], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "        self.gw = nn.ModuleList([G.MLP([dims[l-1] + dims[l], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "        \n",
    "        # Feas and obj output function\n",
    "        self.f_out = G.MLP([2 * dims[self.num_layers-1], 1, 1])\n",
    "\n",
    "        # Sol output function\n",
    "        self.fw_out = G.MLP([3 * dims[self.num_layers-1], 32, 1])\n",
    "\n",
    "    def construct_graph(self, c, A, b, constraints, l, u):\n",
    "        hv = torch.cat((b.unsqueeze(2), constraints.unsqueeze(2)), dim=2)\n",
    "        hw = torch.cat((c.unsqueeze(2), l.unsqueeze(2), u.unsqueeze(2)), dim=2)\n",
    "        E = A\n",
    "        return hv, hw, E\n",
    "\n",
    "    def init_features(self, hv, hw):\n",
    "        hv_0 = []\n",
    "        for i in range(self.num_constraints):\n",
    "            hv_0.append(self.fv_in(hv[:, i]))\n",
    "\n",
    "        hw_0 = []\n",
    "        for j in range(self.num_variables):\n",
    "            hw_0.append(self.fw_in(hw[:, j]))\n",
    "\n",
    "        hv = torch.stack(hv_0, dim=1)\n",
    "        hw = torch.stack(hw_0, dim=1)\n",
    "        return hv, hw\n",
    "\n",
    "    def layer_left(self, hv, hw, E, layer):\n",
    "        \n",
    "        hv_l = []\n",
    "        batch = hv.shape[0]\n",
    "        for i in range(self.num_constraints):\n",
    "            \n",
    "            s = []\n",
    "            for j in range(self.num_variables):\n",
    "                s.append(torch.mul(E[:, i, j, None], self.fw[layer](hw[:, j])))\n",
    "\n",
    "            s = torch.sum(torch.stack(s, dim=1), dim=1)\n",
    "            # s = torch.flatten(s, 1)\n",
    "            # hv_flat = torch.flatten(hv[:, i], 1)\n",
    "\n",
    "            joint = torch.cat((hv[:, i], s), dim=1)\n",
    "\n",
    "            hv_l.append(self.gv[layer](joint))\n",
    "        \n",
    "        hv_l = torch.stack(hv_l, dim=1)\n",
    "\n",
    "        return hv_l\n",
    "    \n",
    "    def layer_right(self, hv, hw, E, layer):\n",
    "        \n",
    "        hw_l = []\n",
    "        for j in range(self.num_variables):\n",
    "\n",
    "            s = []\n",
    "            for i in range(self.num_constraints):\n",
    "                s.append(torch.mul(E[:, i, j, None], self.fv[layer](hv[:, i])))\n",
    "            \n",
    "            s = torch.sum(torch.stack(s, dim=1), dim=1)\n",
    "\n",
    "            joint = torch.cat((hw[:, j], s), dim=1)\n",
    "            \n",
    "            hw_l.append(self.gw[layer](joint))\n",
    "        \n",
    "        hw_l = torch.stack(hw_l, dim=1)\n",
    "\n",
    "        return hw_l\n",
    "    \n",
    "    def single_output(self, hv, hw):\n",
    "        y_out = self.f_out(torch.cat((torch.sum(hv, 1), torch.sum(hw, 1)), dim=1))\n",
    "        return y_out\n",
    "    \n",
    "    def sol_output(self, hv, hw):\n",
    "        sol = []\n",
    "        for j in range(self.num_variables):\n",
    "            joint = torch.cat((torch.sum(hv, 1), torch.sum(hw, 1), hw[:, j]), dim=1)\n",
    "            sol.append(self.fw_out(joint))\n",
    "\n",
    "        sol = torch.stack(sol, dim=1)\n",
    "\n",
    "        return sol[:, :, 0]\n",
    "\n",
    "    def forward(self, c, A, b, constraints, l, u, phi = 'feas'):\n",
    "\n",
    "        hv, hw, E = self.construct_graph(c, A, b, constraints, l, u)\n",
    "        hv, hw = self.init_features(hv, hw)\n",
    "\n",
    "        for l in range(self.num_layers-1):\n",
    "            old_hv = hv\n",
    "            hv = self.layer_left(hv, hw, E, l)\n",
    "            hw = self.layer_right(old_hv, hw, E, l)\n",
    "\n",
    "        if phi == 'feas':\n",
    "            output = self.single_output(hv,hw)\n",
    "            bins = [1 if elem >= 1/2 else 0 for elem in output]\n",
    "            return torch.tensor(bins, dtype=torch.int).to(device)\n",
    "        \n",
    "        elif phi == 'obj':\n",
    "            return self.single_output(hv,hw)\n",
    "        \n",
    "        elif phi == 'sol':\n",
    "            return self.sol_output(hv,hw)\n",
    "        \n",
    "        else:\n",
    "            return \"Please, choose one type of function: feas, obj or sol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_constraints = 2\n",
    "num_variables = 5\n",
    "batch_size = 10\n",
    "N = 100\n",
    "out_func = 'sol'\n",
    "\n",
    "data_train = []\n",
    "for i in range(N):\n",
    "    c, A, b, constraints, l, u, solution, feasibility = generate_and_solve_batches(batch_size,\n",
    "                                                                                   num_variables,\n",
    "                                                                                   num_constraints,\n",
    "                                                                                   out_func)\n",
    "    data_train.append([c, A, b, constraints, l, u, solution, feasibility])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, c, A, b, constraint, l, u, sol, feas, out_func, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model.forward(c, A, b, constraint, l, u, out_func)\n",
    "\n",
    "    #print(feas, out)\n",
    "    loss = nn.MSELoss()\n",
    "    if out_func == 'feas':\n",
    "        loss = loss(out, feas)\n",
    "    elif out_func == 'obj':\n",
    "        loss = loss(out, c.T @ sol)\n",
    "    else:\n",
    "        loss = loss(out, sol)\n",
    "        #print(f\"Out: {out} | Sol: {sol}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training model\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.30664158: 100%|█████████████████████████████████████████████████████████████████████████████| 200/200 [13:50<00:00,  4.15s/it]\n"
     ]
    }
   ],
   "source": [
    "device = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LPGNN(num_constraints, num_variables).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "out_func = 'sol'\n",
    "\n",
    "# training model\n",
    "print(\"\\n\\n\")\n",
    "print(\"Training model\")\n",
    "print(\"\\n\\n\")\n",
    "epochs = 200\n",
    "pbar = tqdm(range(epochs))\n",
    "\n",
    "for epoch in pbar:\n",
    "    for batch in data_train:\n",
    "        c, A, b, constraints, l, u, sol, feas = batch\n",
    "        c = c.to(device)\n",
    "        A = A.to(device)\n",
    "        b = b.to(device)\n",
    "        constraints = constraints.to(device)\n",
    "        l = l.to(device)\n",
    "        u = u.to(device)\n",
    "        sol = sol.to(device)\n",
    "        feas = feas.to(device)\n",
    "        loss = train(model, c, A, b, constraints, l, u, sol, feas, out_func, optimizer)\n",
    "        pbar.set_description(f\"%.8f\" % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Testing model\n",
      "\n",
      "\n",
      "\n",
      "Loss: 36.37246322631836\n",
      "Loss: 48.16724395751953\n",
      "Loss: 45.39456558227539\n",
      "Loss: 52.59383010864258\n",
      "Loss: 56.58796310424805\n",
      "Loss: 73.28307342529297\n",
      "Loss: 51.76296615600586\n",
      "Loss: 34.77534484863281\n",
      "Loss: 37.14517593383789\n",
      "Loss: 92.34169006347656\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "print(\"\\n\\n\")\n",
    "print(\"Testing model\")\n",
    "print(\"\\n\\n\")\n",
    "batch_size = 5\n",
    "M = 10\n",
    "\n",
    "data_test = []\n",
    "for i in range(M):\n",
    "    c, A, b, constraints, l, u, solution, feasibility = generate_and_solve_batches(batch_size,\n",
    "                                                                                   num_variables,\n",
    "                                                                                   num_constraints,\n",
    "                                                                                   out_func)\n",
    "    data_test.append([c, A, b, constraints, l, u, solution, feasibility])\n",
    "\n",
    "def test(model, c, A, b, constraint, l, u, sol, feas, out_func):\n",
    "    out = model.forward(c, A, b, constraint, l, u, out_func)\n",
    "    loss = nn.MSELoss()\n",
    "    if out_func == 'feas':\n",
    "        loss = loss(out, feas)\n",
    "    elif out_func == 'obj':\n",
    "        loss = loss(out, c.T @ sol)\n",
    "    else:\n",
    "        loss = loss(out, sol)\n",
    "    return loss\n",
    "\n",
    "\n",
    "for batch in data_test:\n",
    "    c, A, b, constraints, l, u, sol, feas = batch\n",
    "    c = c.to(device)\n",
    "    A = A.to(device)\n",
    "    b = b.to(device)\n",
    "    constraints = constraints.to(device)\n",
    "    l = l.to(device)\n",
    "    u = u.to(device)\n",
    "    sol = sol.to(device)\n",
    "    feas = feas.to(device)\n",
    "\n",
    "    loss = test(model, c, A, b, constraints, l, u, sol, feas, out_func)\n",
    "    print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Testing model\n",
      "\n",
      "\n",
      "\n",
      "Loss: 96.2463150024414\n",
      "Loss: 66.35426330566406\n",
      "Loss: 74.11528015136719\n",
      "Loss: 56.03019332885742\n",
      "Loss: 88.99691009521484\n",
      "Loss: 54.71165466308594\n",
      "Loss: 92.4083251953125\n",
      "Loss: 116.81864929199219\n",
      "Loss: 120.50040435791016\n",
      "Loss: 108.02588653564453\n"
     ]
    }
   ],
   "source": [
    "model2 = LPGNN(num_constraints, num_variables).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Test model\n",
    "print(\"\\n\\n\")\n",
    "print(\"Testing model\")\n",
    "print(\"\\n\\n\")\n",
    "batch_size = 5\n",
    "M = 10\n",
    "\n",
    "data_test = []\n",
    "for i in range(M):\n",
    "    c, A, b, constraints, l, u, solution, feasibility = generate_and_solve_batches(batch_size,\n",
    "                                                                                   num_variables,\n",
    "                                                                                   num_constraints,\n",
    "                                                                                   out_func)\n",
    "    data_test.append([c, A, b, constraints, l, u, solution, feasibility])\n",
    "\n",
    "def test(model, c, A, b, constraint, l, u, sol, feas, out_func):\n",
    "    out = model.forward(c, A, b, constraint, l, u, out_func)\n",
    "    loss = nn.MSELoss()\n",
    "    if out_func == 'feas':\n",
    "        loss = loss(out, feas)\n",
    "    elif out_func == 'obj':\n",
    "        loss = loss(out, c.T @ sol)\n",
    "    else:\n",
    "        loss = loss(out, sol)\n",
    "    return loss\n",
    "\n",
    "\n",
    "for batch in data_test:\n",
    "    c, A, b, constraints, l, u, sol, feas = batch\n",
    "    c = c.to(device)\n",
    "    A = A.to(device)\n",
    "    b = b.to(device)\n",
    "    constraints = constraints.to(device)\n",
    "    l = l.to(device)\n",
    "    u = u.to(device)\n",
    "    sol = sol.to(device)\n",
    "    feas = feas.to(device)\n",
    "\n",
    "    loss = test(model2, c, A, b, constraints, l, u, sol, feas, out_func)\n",
    "    print(f\"Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
