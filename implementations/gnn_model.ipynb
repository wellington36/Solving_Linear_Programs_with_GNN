{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric.nn as G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_linear_program(num_variables, num_constraints, lower_bound):\n",
    "    # Generate random coefficients for the objective function\n",
    "    c = np.random.rand(num_variables)\n",
    "\n",
    "    # Generate random coefficients for the constraint matrix\n",
    "    A = np.random.rand(num_constraints, num_variables)\n",
    "\n",
    "    # Generate random right-hand side values for the constraints\n",
    "    b = np.random.rand(num_constraints)\n",
    "\n",
    "    # Generate random lower and upper bounds for the variables\n",
    "    lower_bounds = np.random.choice([lower_bound, np.random.rand()], size=num_variables)\n",
    "    upper_bounds = np.random.choice([lower_bound, np.random.rand()], size=num_variables)\n",
    "\n",
    "    # Set the bounds for the variables\n",
    "    bounds = list(zip(lower_bounds, upper_bounds))\n",
    "\n",
    "    # Generate random constraint types (0 for <= and 1 for =)\n",
    "    constraint_types = np.random.choice([0, 1], size=num_constraints)\n",
    "\n",
    "    # Adjust the constraint matrix and right-hand side based on the constraint types\n",
    "    for i in range(num_constraints):\n",
    "        if constraint_types[i] == 1:  # Equality constraint\n",
    "            A[i] = np.random.choice([0, 1]) * A[i]  # Randomly multiply constraint coefficients by 0 or 1\n",
    "            b[i] = np.dot(A[i], np.random.rand(num_variables))  # Randomly generate a feasible solution\n",
    "\n",
    "    # Return the generated linear program\n",
    "    return c, A, b, constraint_types, bounds\n",
    "\n",
    "\n",
    "def solve_linear_program(c, A, b, constraint_types, bounds):\n",
    "    # Solve the linear program\n",
    "    res = linprog(c, A_ub=A[constraint_types == 0], b_ub=b[constraint_types == 0],\n",
    "                  A_eq=A[constraint_types == 1], b_eq=b[constraint_types == 1], bounds=bounds)\n",
    "\n",
    "    # Return the solution\n",
    "    return res.x, res.status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_solve_batches(num_batches, num_variables, num_constraints, out_func):\n",
    "    batches_c = []\n",
    "    batches_A = []\n",
    "    batches_b = []\n",
    "    batches_constraint_types = []\n",
    "    batches_lower_bounds = []\n",
    "    batches_upper_bounds = []\n",
    "    batches_solutions = []\n",
    "    batches_feasibility = []\n",
    "    \n",
    "    if out_func == 'feas':\n",
    "        for _ in range(num_batches):\n",
    "            c, A, b, constraint_types, bounds = generate_random_linear_program(num_variables,\n",
    "                                                                               num_constraints,\n",
    "                                                                               float('inf'))\n",
    "            solution, feasibility = solve_linear_program(c, A, b, constraint_types, bounds)\n",
    "\n",
    "            lower_bounds, upper_bounds = zip(*bounds)\n",
    "\n",
    "            batches_c.append(torch.tensor(c, dtype=torch.float32))\n",
    "            batches_A.append(torch.tensor(A, dtype=torch.float32))\n",
    "            batches_b.append(torch.tensor(b, dtype=torch.float32))\n",
    "            batches_constraint_types.append(torch.tensor(constraint_types, dtype=torch.float32))\n",
    "            batches_lower_bounds.append(torch.tensor(lower_bounds, dtype=torch.float32))\n",
    "            batches_upper_bounds.append(torch.tensor(upper_bounds, dtype=torch.float32))\n",
    "\n",
    "            if type(solution) != type(None):\n",
    "                batches_solutions.append(torch.tensor(solution, dtype=torch.float32))\n",
    "\n",
    "            else:\n",
    "                batches_solutions.append(torch.zeros(num_variables, dtype=torch.float32))\n",
    "\n",
    "            batches_feasibility.append(torch.tensor(1 if feasibility != 2 else 0, dtype=torch.float32))\n",
    "\n",
    "        return (\n",
    "            torch.stack(batches_c),\n",
    "            torch.stack(batches_A),\n",
    "            torch.stack(batches_b),\n",
    "            torch.stack(batches_constraint_types),\n",
    "            torch.stack(batches_lower_bounds),\n",
    "            torch.stack(batches_upper_bounds),\n",
    "            torch.stack(batches_solutions),\n",
    "            torch.stack(batches_feasibility)\n",
    "        )\n",
    "    else:\n",
    "        while len(batches_c) != num_batches:\n",
    "            c, A, b, constraint_types, bounds = generate_random_linear_program(num_variables,\n",
    "                                                                               num_constraints,\n",
    "                                                                              -1000000.0)\n",
    "            solution, feasibility = solve_linear_program(c, A, b, constraint_types, bounds)\n",
    "            \n",
    "            if (type(solution) == type(None)):\n",
    "                continue\n",
    "            \n",
    "            lower_bounds, upper_bounds = zip(*bounds)\n",
    "\n",
    "            batches_c.append(torch.tensor(c, dtype=torch.float32))\n",
    "            batches_A.append(torch.tensor(A, dtype=torch.float32))\n",
    "            batches_b.append(torch.tensor(b, dtype=torch.float32))\n",
    "            batches_constraint_types.append(torch.tensor(constraint_types, dtype=torch.float32))\n",
    "            batches_lower_bounds.append(torch.tensor(lower_bounds, dtype=torch.float32))\n",
    "            batches_upper_bounds.append(torch.tensor(upper_bounds, dtype=torch.float32))\n",
    "            batches_solutions.append(torch.zeros(num_variables, dtype=torch.float32))\n",
    "            batches_feasibility.append(torch.tensor(1 if feasibility != 2 else 0, dtype=torch.float32))\n",
    "\n",
    "        return (\n",
    "            torch.stack(batches_c),\n",
    "            torch.stack(batches_A),\n",
    "            torch.stack(batches_b),\n",
    "            torch.stack(batches_constraint_types),\n",
    "            torch.stack(batches_lower_bounds),\n",
    "            torch.stack(batches_upper_bounds),\n",
    "            torch.stack(batches_solutions),\n",
    "            torch.stack(batches_feasibility)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LPGNN(nn.Module):\n",
    "    def __init__(self, num_constraints, num_variables):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_constraints = num_constraints\n",
    "        self.num_variables = num_variables\n",
    "\n",
    "        self.num_layers = 5\n",
    "        ints = np.random.randint(1, 10, size=self.num_layers)\n",
    "        dims = [2 ** i for i in ints]\n",
    "        \n",
    "        # Encode the input features into the embedding space\n",
    "        self.fv_in = G.MLP([2, 32, dims[0]])\n",
    "        self.fw_in = G.MLP([3, 32, dims[0]])\n",
    "\n",
    "        # Hidden states of left nodes\n",
    "        self.fv = nn.ModuleList([G.MLP([dims[l-1], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "        self.gv = nn.ModuleList([G.MLP([dims[l-1] + dims[l], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "\n",
    "        # Hidden states of right nodes\n",
    "        self.fw = nn.ModuleList([G.MLP([dims[l-1], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "        self.gw = nn.ModuleList([G.MLP([dims[l-1] + dims[l], 32, dims[l]]) for l in range(1, self.num_layers)])\n",
    "        \n",
    "        # Feas and obj output function\n",
    "        self.f_out = G.MLP([2 * dims[self.num_layers-1], 1, 1])\n",
    "\n",
    "        # Sol output function\n",
    "        self.fw_out = G.MLP([3 * dims[self.num_layers-1], 32, 1])\n",
    "\n",
    "    def construct_graph(self, c, A, b, constraints, l, u):\n",
    "        hv = torch.cat((b.unsqueeze(2), constraints.unsqueeze(2)), dim=2)\n",
    "        hw = torch.cat((c.unsqueeze(2), l.unsqueeze(2), u.unsqueeze(2)), dim=2)\n",
    "        E = A\n",
    "        return hv, hw, E\n",
    "\n",
    "    def init_features(self, hv, hw):\n",
    "        hv_0 = []\n",
    "        for i in range(self.num_constraints):\n",
    "            hv_0.append(self.fv_in(hv[:, i]))\n",
    "\n",
    "        hw_0 = []\n",
    "        for j in range(self.num_variables):\n",
    "            hw_0.append(self.fw_in(hw[:, j]))\n",
    "\n",
    "        hv = torch.stack(hv_0, dim=1)\n",
    "        hw = torch.stack(hw_0, dim=1)\n",
    "        return hv, hw\n",
    "\n",
    "    def layer_left(self, hv, hw, E, layer):\n",
    "        \n",
    "        hv_l = []\n",
    "        batch = hv.shape[0]\n",
    "        for i in range(self.num_constraints):\n",
    "            \n",
    "            s = []\n",
    "            for j in range(self.num_variables):\n",
    "                s.append(torch.mul(E[:, i, j, None], self.fw[layer](hw[:, j])))\n",
    "\n",
    "            s = torch.sum(torch.stack(s, dim=1), dim=1)\n",
    "            # s = torch.flatten(s, 1)\n",
    "            # hv_flat = torch.flatten(hv[:, i], 1)\n",
    "\n",
    "            joint = torch.cat((hv[:, i], s), dim=1)\n",
    "\n",
    "            hv_l.append(self.gv[layer](joint))\n",
    "        \n",
    "        hv_l = torch.stack(hv_l, dim=1)\n",
    "\n",
    "        return hv_l\n",
    "    \n",
    "    def layer_right(self, hv, hw, E, layer):\n",
    "        \n",
    "        hw_l = []\n",
    "        for j in range(self.num_variables):\n",
    "\n",
    "            s = []\n",
    "            for i in range(self.num_constraints):\n",
    "                s.append(torch.mul(E[:, i, j, None], self.fv[layer](hv[:, i])))\n",
    "            \n",
    "            s = torch.sum(torch.stack(s, dim=1), dim=1)\n",
    "\n",
    "            joint = torch.cat((hw[:, j], s), dim=1)\n",
    "            \n",
    "            hw_l.append(self.gw[layer](joint))\n",
    "        \n",
    "        hw_l = torch.stack(hw_l, dim=1)\n",
    "\n",
    "        return hw_l\n",
    "    \n",
    "    def single_output(self, hv, hw):\n",
    "        y_out = self.f_out(torch.cat((torch.sum(hv, 1), torch.sum(hw, 1)), dim=1))\n",
    "        return y_out\n",
    "    \n",
    "    def sol_output(self, hv, hw):\n",
    "        sol = []\n",
    "        for j in range(self.num_variables):\n",
    "            joint = torch.cat((torch.sum(hv, 1), torch.sum(hw, 1), hw[:, j]), dim=1)\n",
    "            sol.append(self.fw_out(joint))\n",
    "\n",
    "        sol = torch.stack(sol, dim=1)\n",
    "\n",
    "        return sol[:, :, 0]\n",
    "\n",
    "    def forward(self, c, A, b, constraints, l, u, phi = 'feas'):\n",
    "\n",
    "        hv, hw, E = self.construct_graph(c, A, b, constraints, l, u)\n",
    "        hv, hw = self.init_features(hv, hw)\n",
    "\n",
    "        for l in range(self.num_layers-1):\n",
    "            old_hv = hv\n",
    "            hv = self.layer_left(hv, hw, E, l)\n",
    "            hw = self.layer_right(old_hv, hw, E, l)\n",
    "\n",
    "        if phi == 'feas':\n",
    "            output = self.single_output(hv,hw)\n",
    "            bins = [1 if elem >= 1/2 else 0 for elem in output]\n",
    "            return bins\n",
    "        \n",
    "        elif phi == 'obj':\n",
    "            return self.single_output(hv,hw)\n",
    "        \n",
    "        elif phi == 'sol':\n",
    "            return self.sol_output(hv,hw)\n",
    "        \n",
    "        else:\n",
    "            return \"Please, choose one type of function: feas, obj or sol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_constraints = 10\n",
    "num_variables = 50\n",
    "batch_size = 2\n",
    "out_func = 'feas'\n",
    "\n",
    "data_train = []\n",
    "for i in range(50):\n",
    "    c, A, b, constraints, l, u, solution, feasibility = generate_and_solve_batches(batch_size,\n",
    "                                                                                   num_variables,\n",
    "                                                                                   num_constraints,\n",
    "                                                                                   out_func)\n",
    "    data_train.append([c, A, b, constraints, l, u, solution, feasibility])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(c, A, b, constraint, l, u, sol, feas, out_func, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(c, A, b, constraint, l, u, out_func)\n",
    "    print(feas, out)\n",
    "    loss = nn.MSELoss()\n",
    "    if out_func == 'feas':\n",
    "        loss = loss(out, feas)\n",
    "    elif out_func == 'obj':\n",
    "        loss = loss(out, c.T @ sol)\n",
    "    else:\n",
    "        loss = loss(out, sol)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan]], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan:   0%|                                             | 0/1000 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan]], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan:   0%|                                             | 0/1000 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan]], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan:   0%|                                             | 0/1000 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan]], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan:   0%|                                             | 0/1000 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m sol \u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m feas \u001b[38;5;241m=\u001b[39m feas\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m train(c, A, b, constraints, l, u, sol, feas, out_func, optimizer)\n\u001b[1;32m     25\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%.8f\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m loss)\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(c, A, b, constraint, l, u, sol, feas, out_func, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(c, A, b, constraint, l, u, sol, feas, out_func, optimizer):\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(c, A, b, constraint, l, u, out_func)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feas, out)\n\u001b[1;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 112\u001b[0m, in \u001b[0;36mLPGNN.forward\u001b[0;34m(self, c, A, b, constraints, l, u, phi)\u001b[0m\n\u001b[1;32m    110\u001b[0m     old_hv \u001b[38;5;241m=\u001b[39m hv\n\u001b[1;32m    111\u001b[0m     hv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_left(hv, hw, E, l)\n\u001b[0;32m--> 112\u001b[0m     hw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_right(old_hv, hw, E, l)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phi \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeas\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    115\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_output(hv,hw)\n",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m, in \u001b[0;36mLPGNN.layer_right\u001b[0;34m(self, hv, hw, E, layer)\u001b[0m\n\u001b[1;32m     76\u001b[0m s \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_constraints):\n\u001b[0;32m---> 78\u001b[0m     s\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mmul(E[:, i, j, \u001b[38;5;28;01mNone\u001b[39;00m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfv[layer](hv[:, i])))\n\u001b[1;32m     80\u001b[0m s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mstack(s, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m joint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hw[:, j], s), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch_geometric/nn/models/mlp.py:204\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x, return_emb)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_first:\n\u001b[1;32m    203\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[0;32m--> 204\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_first:\n\u001b[1;32m    206\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch_geometric/nn/norm/batch_norm.py:87\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_single_element \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m     78\u001b[0m         x,\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mrunning_mean,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(x)\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.anaconda3/envs/cuda/lib/python3.11/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LPGNN(num_constraints, num_variables).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "out_func = 'sol'\n",
    "\n",
    "# training model\n",
    "epochs = 1000\n",
    "pbar = tqdm(range(epochs))\n",
    "\n",
    "for epoch in pbar:\n",
    "    for batch in data_train:\n",
    "        c, A, b, constraints, l, u, sol, feas = batch\n",
    "        c = c.to(device)\n",
    "        A = A.to(device)\n",
    "        b = b.to(device)\n",
    "        constraints = constraints.to(device)\n",
    "        l = l.to(device)\n",
    "        u = u.to(device)\n",
    "        sol = sol.to(device)\n",
    "        feas = feas.to(device)\n",
    "        loss = train(c, A, b, constraints, l, u, sol, feas, out_func, optimizer)\n",
    "        pbar.set_description(f\"%.8f\" % loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
